import os
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModel
import torch
import faiss
import streamlit as st
import google.generativeai as genai

# ê²½ë¡œ ì„¤ì •
data_path = './data'
module_path = './modules'

# Gemini ëª¨ë¸ ì„¤ì •
GOOGLE_API_KEY = st.secrets["API_KEY"]

genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel("gemini-1.5-flash")

# ë°ì´í„° ë¡œë“œ
df = pd.read_csv(os.path.join(data_path, "JEJU_DATA.csv"), encoding='cp949')
df_tour = pd.read_csv(os.path.join(data_path, "JEJU_TOUR.csv"), encoding='cp949')
text_tour = df_tour['text'].tolist()

# ìµœì‹ ì—°ì›” ë°ì´í„°ë§Œ ì‚¬ìš©
df = df.loc[df.groupby('ê°€ë§¹ì ëª…')['ê¸°ì¤€ì—°ì›”'].idxmax()].reset_index(drop=True)

# Streamlit App UI

st.set_page_config(page_title="ğŸŠì œì£¼ë„ ë§›ì§‘ ì¶”ì²œ")

# Replicate Credentials
with st.sidebar:
    st.title("**ğŸŠì œì£¼ë„ ë§›ì§‘ ì¶”ì²œ**")

    st.write("")
    st.markdown("""
        <style>
        .sidebar-text {
        color: #FFEC9D;
        font-size: 18px;
        font-weight: bold;
        }
     </style>
     """, unsafe_allow_html=True)

    st.sidebar.markdown('<p class="sidebar-text">ğŸ’µí¬ë§ ê°€ê²©ëŒ€ëŠ” ì–´ë–»ê²Œ ë˜ì‹œë‚˜ìš”??</p>', unsafe_allow_html=True)


    price = st.sidebar.selectbox("", ['ğŸ‘Œ ìƒê´€ ì—†ìŒ','ğŸ˜ ìµœê³ ê°€', 'ğŸ’¸ ê³ ê°€', 'ğŸ’° í‰ê·  ê°€ê²©ëŒ€', 'ğŸ’µ ì¤‘ì €ê°€', 'ğŸ˜‚ ì €ê°€'], key="price")

    if price == 'ğŸ‘Œ ìƒê´€ ì—†ìŒ':
        price = 'ìƒê´€ ì—†ìŒ'
    elif price == 'ğŸ˜ ìµœê³ ê°€':
        price = 'ìµœê³ ê°€'
    elif price == 'ğŸ’¸ ê³ ê°€':
        price = 'ê³ ê°€'
    elif price == 'ğŸ’° í‰ê·  ê°€ê²©ëŒ€':
        price = 'í‰ê·  ê°€ê²©ëŒ€'
    elif price == 'ğŸ’µ ì¤‘ì €ê°€':
        price = 'ì¤‘ì €ê°€'
    elif price == 'ğŸ˜‚ ì €ê°€':
        price = 'ì €ê°€'

    st.markdown(
        """
         <style>
         [data-testid="stSidebar"] {
         background-color: #ff9900;
         }
         </style>
        """, unsafe_allow_html=True)
    st.write("")

st.title("ì–´ì„œ ì™€ìš©!ğŸ‘‹")
st.subheader("ì¸ê¸° ìˆëŠ” :orange[ì œì£¼ ë§›ì§‘]ğŸ½ï¸ğŸ˜ í›„íšŒëŠ” ì—†ì„ê±¸?!")

st.write("")

st.write("#í‘ë¼ì§€ #ì œì²  ìƒì„ íšŒ #í•´ë¬¼ë¼ë©´ #ìŠ¤í…Œì´í¬ #í•œì‹ #ì¤‘ì‹ #ì–‘ì‹ #ì¼ì‹ #í‘ë°±ìš”ë¦¬ì‚¬..ğŸ¤¤")

st.write("")

# ì´ë¯¸ì§€ ì¶”ê°€
image_path = "https://pimg.mk.co.kr/news/cms/202409/22/news-p.v1.20240922.a626061476c54127bbe4beb0aa12d050_P1.png"
image_html = f"""
<div style="display: flex; justify-content: center;">
    <img src="{image_path}" alt="centered image" width="70%">
</div>
"""
st.markdown(image_html, unsafe_allow_html=True)

st.write("")

# ëŒ€í™” ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë–¤ ì‹ë‹¹ ì°¾ìœ¼ì‹œë‚˜ìš”?? ìœ„ì¹˜, ì—…ì¢… ë“±ì„ ì•Œë ¤ì£¼ì‹œë©´ ìµœê³ ì˜ ë§›ì§‘ ì¶”ì²œí•´ë“œë¦´ê²Œìš”!"}]

# ë©”ì‹œì§€ ì¶œë ¥
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# ì±— ê¸°ë¡ ì´ˆê¸°í™” ë²„íŠ¼
def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë–¤ ì‹ë‹¹ ì°¾ìœ¼ì‹œë‚˜ìš”?? ìœ„ì¹˜, ì—…ì¢… ë“±ì„ ì•Œë ¤ì£¼ì‹œë©´ ìµœê³ ì˜ ë§›ì§‘ ì¶”ì²œí•´ë“œë¦´ê²Œìš”!"}]
st.sidebar.button('ëŒ€í™” ì´ˆê¸°í™” ğŸ”„', on_click=clear_chat_history)



# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"

# Hugging Face ì„ë² ë”© ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "jhgan/ko-sroberta-multitask"
tokenizer = AutoTokenizer.from_pretrained(model_name)
embedding_model = AutoModel.from_pretrained(model_name).to(device)

# FAISS ì¸ë±ìŠ¤ ë¡œë“œ í•¨ìˆ˜
def load_faiss_index(index_path=os.path.join(module_path, 'faiss_index_1.index')):
    if os.path.exists(index_path):
        index = faiss.read_index(index_path)
        return index
    else:
        raise FileNotFoundError(f"ì¸ë±ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {index_path}")

# í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
def embed_text(text):
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)
    with torch.no_grad():
        embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1)
    return embeddings.squeeze().cpu().numpy()

# í…ìŠ¤íŠ¸ ì„ë² ë”© ë¡œë“œ
embeddings = np.load(os.path.join(module_path, 'embeddings_array_file_1.npy'))
embeddings_tour = np.load(os.path.join(module_path, 'embeddings_tour_array_file_1.npy'))

# FAISSë¥¼ í™œìš©í•œ ì‘ë‹µ ìƒì„±
def generate_response_with_faiss(question, df, embeddings, model, df_tour, embeddings_tour,max_count=10, k=3, print_prompt=True):
    index = load_faiss_index()
    query_embedding = embed_text(question).reshape(1, -1)
    distances, indices = index.search(query_embedding, k * 3)

    index_tour = load_faiss_index(index_path=os.path.join(module_path, 'faiss_tour_index_1.index'))
    query_embedding_tour = embed_text(question).reshape(1, -1)
    distances_tour, indices_tour = index_tour.search(query_embedding_tour, 1)

    filtered_df = df.iloc[indices[0, :]].reset_index(drop=True)
    filtered_df_tour = df_tour.iloc[indices_tour[0, :]].reset_index(drop=True)

    # í¬ë§ ê°€ê²©ëŒ€ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê°€ê²Œë“¤ë§Œ í•„í„°ë§
    if price == 'ìƒê´€ ì—†ìŒ':
        filtered_df = filtered_df
    elif price == 'ìµœê³ ê°€':
        filtered_df = filtered_df[filtered_df['ê±´ë‹¹í‰ê· ì´ìš©ê¸ˆì•¡êµ¬ê°„'].str.startswith('6')].reset_index(drop=True)
    elif price == 'ê³ ê°€':
        filtered_df = filtered_df[filtered_df['ê±´ë‹¹í‰ê· ì´ìš©ê¸ˆì•¡êµ¬ê°„'].str.startswith('5')].reset_index(drop=True)
    elif price == 'í‰ê·  ê°€ê²©ëŒ€':
        filtered_df = filtered_df[filtered_df['ê±´ë‹¹í‰ê· ì´ìš©ê¸ˆì•¡êµ¬ê°„'].str.startswith('3'or '4')].reset_index(drop=True)
    elif price == 'ì €ê°€':
        filtered_df = filtered_df[filtered_df['ê±´ë‹¹í‰ê· ì´ìš©ê¸ˆì•¡êµ¬ê°„'].str.startswith('2')].reset_index(drop=True)
    elif price == 'ìµœì €ê°€':
        filtered_df = filtered_df[filtered_df['ê±´ë‹¹í‰ê· ì´ìš©ê¸ˆì•¡êµ¬ê°„'].str.startswith('1')].reset_index(drop=True)


    filtered_df = filtered_df.reset_index(drop=True).head(k * 3)

    if filtered_df.empty:
        return "ì§ˆë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ” ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤."

    reference_info = "\n".join(filtered_df['text'])
    reference_tour = "\n".join(filtered_df_tour['text'])

    prompt = f"""ì§ˆë¬¸: {question}\nëŒ€ë‹µì‹œ í•„ìš”í•œ ë‚´ìš©: ê·¼ì²˜ ìŒì‹ì ì„ ì¶”ì²œí• ë•ŒëŠ” ì§ˆë¬¸ì— ì£¼ì†Œì— ëŒ€í•œ ì •ë³´ê°€ ìˆë‹¤ë©´ ìŒì‹ì ì˜ ì£¼ì†Œê°€ ë¹„ìŠ·í•œì§€ í™•ì¸í•´.\nì°¨ë¡œ ì´ë™ì‹œê°„ì´ ì–¼ë§ˆì¸ì§€ ì•Œë ¤ì¤˜. ì¶”ì²œí•´ì¤„ë•Œ ì´ë™ì‹œê°„ì„ ê³ ë ¤í•´ì„œ ë‹µë³€í•´ì¤˜.\nê°€ë§¹ì ì—…ì¢…ì´ ì»¤í”¼ì¸ ê°€ê²ŒëŠ” ì—…ì¢…ì´ ì¹´í˜ì•¼. \nëŒ€ë‹µí•´ì¤„ë•Œ ì—…ì¢…ë³„ë¡œ ê°€ëŠ¥í•˜ë©´ í•˜ë‚˜ì”© ì¶”ì²œí•´ì¤˜. ê·¸ë¦¬ê³  ì¶”ê°€ì ìœ¼ë¡œ ê·¸ ì¤‘ì—ì„œ ê°€ë§¹ì ê°œì ì¼ìê°€ ì˜¤ë˜ë˜ê³  ì´ìš©ê±´ìˆ˜ê°€ ë§ì€ ìŒì‹ì (ì˜¤ë˜ëœë§›ì§‘)ê³¼ ê°€ë§¹ì ê°œì ì¼ìê°€ ìµœê·¼ì´ê³  ì´ìš©ê±´ìˆ˜ê°€ ë§ì€ ìŒì‹ì (ìƒˆë¡œìš´ë§›ì§‘)ì„ ê°ê° ì¶”ì²œí•´ì¤¬ìœ¼ë©´ ì¢‹ê² ì–´.\nì°¸ê³ í•  ì •ë³´: {reference_info}\nì°¸ê³ í•  ê´€ê´‘ì§€ ì •ë³´: {reference_tour}\nì‘ë‹µ:"""

    if print_prompt:
        print('-----------------------------'*3)
        print(prompt)
        print('-----------------------------'*3)

    response = model.generate_content(prompt)
    return response.text if hasattr(response, 'text') else response

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„±
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

# ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("ìƒê° ì¤‘..."):
            response = generate_response_with_faiss(prompt, df, embeddings, model, df_tour, embeddings_tour)
            st.write(response)
    st.session_state.messages.append({"role": "assistant", "content": response})